#!/usr/bin/env python3
"""
automation/blog_automation.py
ì™„ì „í•œ ë¸”ë¡œê·¸ ìë™í™” ìŠ¤í¬ë¦½íŠ¸ - ê¸°ì¡´ automation í´ë”ì— ì¶”ê°€
"""

import os
import sys
import yaml
import json
import requests
import subprocess
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any
import hashlib
import shutil
from PIL import Image
import markdown

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('automation/blog_automation.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class BlogAutomation:
    """ë¸”ë¡œê·¸ ìë™í™” ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self, config_path: str = "_config.yml"):
        self.config_path = config_path
        self.site_config = self.load_site_config()
        self.github_token = os.getenv('GITHUB_TOKEN')
        self.repo_owner = 'jkimjoon70'
        self.repo_name = 'jkimjoon70.github.io'
        self.site_url = 'https://jkimjoon70.github.io'
        
        # ë””ë ‰í† ë¦¬ ì„¤ì •
        self.base_dir = Path('.')
        self.posts_dir = self.base_dir / '_posts'
        self.assets_dir = self.base_dir / 'assets'
        self.images_dir = self.assets_dir / 'images'
        self.automation_dir = self.base_dir / 'automation'
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        self.automation_dir.mkdir(exist_ok=True)
        self.images_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info("ğŸ¤– Blog Automation initialized")

    def load_site_config(self) -> Dict[str, Any]:
        """Jekyll ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            logger.info("âœ… Site configuration loaded")
            return config
        except FileNotFoundError:
            logger.warning(f"âš ï¸ Configuration file {self.config_path} not found")
            return {}
        except yaml.YAMLError as e:
            logger.error(f"âŒ Error parsing YAML config: {e}")
            return {}

    def check_site_health(self) -> Dict[str, Any]:
        """ì‚¬ì´íŠ¸ ìƒíƒœ ì¢…í•© ì ê²€"""
        logger.info("ğŸ¥ Starting comprehensive site health check...")
        
        health_report = {
            'timestamp': datetime.now().isoformat(),
            'site_availability': self.check_site_availability(),
            'performance': self.check_site_performance(),
            'seo_health': self.check_seo_health(),
            'content_integrity': self.check_content_integrity(),
            'security_scan': self.check_security(),
            'build_status': self.check_build_status()
        }
        
        # ê±´ê°•ë„ ì ìˆ˜ ê³„ì‚°
        health_report['overall_score'] = self.calculate_health_score(health_report)
        
        # ë³´ê³ ì„œ ì €ì¥
        self.save_health_report(health_report)
        
        logger.info(f"ğŸ¯ Overall health score: {health_report['overall_score']}/100")
        return health_report

    def check_site_availability(self) -> Dict[str, Any]:
        """ì‚¬ì´íŠ¸ ì ‘ê·¼ì„± í™•ì¸"""
        try:
            start_time = datetime.now()
            response = requests.get(self.site_url, timeout=10)
            end_time = datetime.now()
            
            response_time = (end_time - start_time).total_seconds() * 1000
            
            return {
                'status': 'online' if response.status_code == 200 else 'issues',
                'status_code': response.status_code,
                'response_time_ms': round(response_time, 2),
                'headers': dict(response.headers),
                'ssl_valid': response.url.startswith('https://'),
                'timestamp': datetime.now().isoformat()
            }
        except requests.RequestException as e:
            logger.error(f"âŒ Site availability check failed: {e}")
            return {
                'status': 'offline',
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }

    def check_site_performance(self) -> Dict[str, Any]:
        """ì‚¬ì´íŠ¸ ì„±ëŠ¥ ë¶„ì„"""
        logger.info("âš¡ Analyzing site performance...")
        
        performance_data = {
            'lighthouse_score': self.run_lighthouse_audit(),
            'page_size': self.analyze_page_size(),
            'load_time_history': self.get_load_time_history(),
            'optimization_suggestions': []
        }
        
        # ìµœì í™” ì œì•ˆ ìƒì„±
        if performance_data['lighthouse_score'] and performance_data['lighthouse_score'] < 90:
            performance_data['optimization_suggestions'].append("Consider optimizing images and CSS")
        
        return performance_data

    def run_lighthouse_audit(self) -> Optional[int]:
        """Lighthouse ì„±ëŠ¥ ê°ì‚¬ ì‹¤í–‰"""
        try:
            cmd = [
                'lighthouse', self.site_url,
                '--output=json',
                '--output-path=automation/lighthouse-report.json',
                '--chrome-flags="--headless --no-sandbox"',
                '--quiet'
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)
            
            if result.returncode == 0 and os.path.exists('automation/lighthouse-report.json'):
                with open('automation/lighthouse-report.json', 'r') as f:
                    report = json.load(f)
                    performance_score = report.get('lhr', {}).get('categories', {}).get('performance', {}).get('score', 0)
                    return int(performance_score * 100) if performance_score else None
            
        except (subprocess.TimeoutExpired, FileNotFoundError, json.JSONDecodeError) as e:
            logger.warning(f"âš ï¸ Lighthouse audit failed: {e}")
        
        return None

    def analyze_page_size(self) -> Dict[str, Any]:
        """í˜ì´ì§€ í¬ê¸° ë¶„ì„"""
        try:
            response = requests.get(self.site_url)
            html_size = len(response.content)
            
            # CSS íŒŒì¼ í¬ê¸° í™•ì¸
            css_size = 0
            css_files = self.find_css_files()
            for css_file in css_files:
                try:
                    css_response = requests.get(f"{self.site_url}/{css_file}")
                    css_size += len(css_response.content)
                except:
                    pass
            
            return {
                'html_size_kb': round(html_size / 1024, 2),
                'css_size_kb': round(css_size / 1024, 2),
                'total_size_kb': round((html_size + css_size) / 1024, 2)
            }
        except Exception as e:
            logger.error(f"âŒ Page size analysis failed: {e}")
            return {}

    def find_css_files(self) -> List[str]:
        """CSS íŒŒì¼ ëª©ë¡ ì°¾ê¸°"""
        css_files = []
        css_dir = self.assets_dir / 'css'
        
        if css_dir.exists():
            for css_file in css_dir.glob('*.css'):
                css_files.append(f"assets/css/{css_file.name}")
        
        return css_files

    def get_load_time_history(self) -> List[Dict[str, Any]]:
        """ë¡œë“œ ì‹œê°„ íˆìŠ¤í† ë¦¬ ì¡°íšŒ"""
        history_file = self.automation_dir / 'performance_history.json'
        
        if history_file.exists():
            try:
                with open(history_file, 'r') as f:
                    return json.load(f)
            except json.JSONDecodeError:
                return []
        
        return []

    def check_seo_health(self) -> Dict[str, Any]:
        """SEO ìƒíƒœ ì ê²€"""
        logger.info("ğŸ” Checking SEO health...")
        
        seo_report = {
            'sitemap_exists': self.check_sitemap(),
            'robots_txt_exists': self.check_robots_txt(),
            'meta_tags_analysis': self.analyze_meta_tags(),
            'structured_data': self.check_structured_data(),
            'internal_links': self.check_internal_links(),
            'page_titles': self.analyze_page_titles()
        }
        
        return seo_report

    def check_sitemap(self) -> Dict[str, Any]:
        """ì‚¬ì´íŠ¸ë§µ ì¡´ì¬ ë° ìœ íš¨ì„± í™•ì¸"""
        sitemap_url = f"{self.site_url}/sitemap.xml"
        
        try:
            response = requests.get(sitemap_url)
            if response.status_code == 200:
                # XML íŒŒì‹±í•˜ì—¬ URL ê°œìˆ˜ í™•ì¸
                import xml.etree.ElementTree as ET
                root = ET.fromstring(response.content)
                url_count = len(root.findall('.//{http://www.sitemaps.org/schemas/sitemap/0.9}url'))
                
                return {
                    'exists': True,
                    'accessible': True,
                    'url_count': url_count,
                    'last_modified': response.headers.get('Last-Modified'),
                    'size_kb': round(len(response.content) / 1024, 2)
                }
            else:
                return {'exists': False, 'status_code': response.status_code}
        except Exception as e:
            return {'exists': False, 'error': str(e)}

    def check_robots_txt(self) -> Dict[str, Any]:
        """robots.txt í™•ì¸"""
        robots_url = f"{self.site_url}/robots.txt"
        
        try:
            response = requests.get(robots_url)
            return {
                'exists': response.status_code == 200,
                'content': response.text if response.status_code == 200 else None,
                'size': len(response.content) if response.status_code == 200 else 0
            }
        except Exception as e:
            return {'exists': False, 'error': str(e)}

    def analyze_meta_tags(self) -> Dict[str, Any]:
        """ë©”íƒ€ íƒœê·¸ ë¶„ì„"""
        try:
            response = requests.get(self.site_url)
            html_content = response.text
            
            # ê¸°ë³¸ ë©”íƒ€ íƒœê·¸ í™•ì¸
            meta_analysis = {
                'has_title': '<title>' in html_content,
                'has_description': 'name="description"' in html_content,
                'has_keywords': 'name="keywords"' in html_content,
                'has_og_tags': 'property="og:' in html_content,
                'has_twitter_cards': 'name="twitter:' in html_content,
                'has_canonical': 'rel="canonical"' in html_content
            }
            
            return meta_analysis
        except Exception as e:
            logger.error(f"âŒ Meta tags analysis failed: {e}")
            return {}

    def check_structured_data(self) -> Dict[str, Any]:
        """êµ¬ì¡°í™”ëœ ë°ì´í„° í™•ì¸"""
        try:
            response = requests.get(self.site_url)
            html_content = response.text
            
            return {
                'has_json_ld': 'application/ld+json' in html_content,
                'has_microdata': 'itemscope' in html_content,
                'has_rdfa': 'typeof=' in html_content
            }
        except Exception as e:
            return {'error': str(e)}

    def check_internal_links(self) -> Dict[str, Any]:
        """ë‚´ë¶€ ë§í¬ í™•ì¸"""
        try:
            # htmlproofer ì‚¬ìš©í•˜ì—¬ ë§í¬ ì²´í¬
            cmd = ['bundle', 'exec', 'htmlproofer', './_site', '--disable-external', '--check-html']
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)
            
            return {
                'check_passed': result.returncode == 0,
                'output': result.stdout,
                'errors': result.stderr if result.returncode != 0 else None
            }
        except Exception as e:
            return {'error': str(e)}

    def analyze_page_titles(self) -> Dict[str, Any]:
        """í˜ì´ì§€ ì œëª© ë¶„ì„"""
        titles_analysis = {
            'post_titles': [],
            'duplicate_titles': [],
            'missing_titles': []
        }
        
        # _posts ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  í¬ìŠ¤íŠ¸ í™•ì¸
        if self.posts_dir.exists():
            for post_file in self.posts_dir.glob('*.md'):
                try:
                    with open(post_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                        
                    # Front matterì—ì„œ title ì¶”ì¶œ
                    if content.startswith('---'):
                        front_matter = content.split('---')[1]
                        front_matter_data = yaml.safe_load(front_matter)
                        title = front_matter_data.get('title')
                        
                        if title:
                            titles_analysis['post_titles'].append({
                                'file': post_file.name,
                                'title': title,
                                'length': len(title)
                            })
                        else:
                            titles_analysis['missing_titles'].append(post_file.name)
                            
                except Exception as e:
                    logger.warning(f"âš ï¸ Error analyzing {post_file}: {e}")
        
        # ì¤‘ë³µ ì œëª© ì°¾ê¸°
        titles = [item['title'] for item in titles_analysis['post_titles']]
        titles_analysis['duplicate_titles'] = list(set([title for title in titles if titles.count(title) > 1]))
        
        return titles_analysis

    def check_content_integrity(self) -> Dict[str, Any]:
        """ì½˜í…ì¸  ë¬´ê²°ì„± í™•ì¸"""
        logger.info("ğŸ“ Checking content integrity...")
        
        integrity_report = {
            'posts_count': self.count_posts(),
            'images_analysis': self.analyze_images(),
            'broken_links': self.find_broken_links(),
            'missing_assets': self.find_missing_assets(),
            'content_quality': self.analyze_content_quality()
        }
        
        return integrity_report

    def count_posts(self) -> Dict[str, int]:
        """í¬ìŠ¤íŠ¸ ê°œìˆ˜ ê³„ì‚°"""
        counts = {
            'total_posts': 0,
            'published_posts': 0,
            'draft_posts': 0,
            'recent_posts': 0  # ìµœê·¼ 30ì¼
        }
        
        if self.posts_dir.exists():
            recent_date = datetime.now() - timedelta(days=30)
            
            for post_file in self.posts_dir.glob('*.md'):
                counts['total_posts'] += 1
                
                try:
                    with open(post_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    if content.startswith('---'):
                        front_matter = content.split('---')[1]
                        front_matter_data = yaml.safe_load(front_matter)
                        
                        # ë°œí–‰ ìƒíƒœ í™•ì¸
                        if not front_matter_data.get('draft', False):
                            counts['published_posts'] += 1
                        else:
                            counts['draft_posts'] += 1
                        
                        # ìµœê·¼ í¬ìŠ¤íŠ¸ í™•ì¸
                        post_date = front_matter_data.get('date')
                        if post_date and isinstance(post_date, datetime) and post_date > recent_date:
                            counts['recent_posts'] += 1
                            
                except Exception as e:
                    logger.warning(f"âš ï¸ Error reading {post_file}: {e}")
        
        return counts

    def analyze_images(self) -> Dict[str, Any]:
        """ì´ë¯¸ì§€ ë¶„ì„"""
        image_analysis = {
            'total_images': 0,
            'total_size_mb': 0,
            'large_images': [],  # 1MB ì´ìƒ
            'unoptimized_images': [],
            'missing_alt_text': []
        }
        
        if self.images_dir.exists():
            for img_file in self.images_dir.rglob('*'):
                if img_file.is_file() and img_file.suffix.lower() in ['.jpg', '.jpeg', '.png', '.gif', '.webp']:
                    image_analysis['total_images'] += 1
                    
                    # íŒŒì¼ í¬ê¸° í™•ì¸
                    file_size = img_file.stat().st_size
                    image_analysis['total_size_mb'] += file_size / (1024 * 1024)
                    
                    if file_size > 1024 * 1024:  # 1MB ì´ìƒ
                        image_analysis['large_images'].append({
                            'file': str(img_file.relative_to(self.base_dir)),
                            'size_mb': round(file_size / (1024 * 1024), 2)
                        })
                    
                    # ì´ë¯¸ì§€ ìµœì í™” ê°€ëŠ¥ì„± í™•ì¸
                    if img_file.suffix.lower() in ['.png', '.jpg', '.jpeg']:
                        try:
                            with Image.open(img_file) as img:
                                if img.mode == 'RGBA' and img_file.suffix.lower() in ['.jpg', '.jpeg']:
                                    image_analysis['unoptimized_images'].append(str(img_file.relative_to(self.base_dir)))
                        except Exception:
                            pass
        
        image_analysis['total_size_mb'] = round(image_analysis['total_size_mb'], 2)
        return image_analysis

    def find_broken_links(self) -> List[str]:
        """ê¹¨ì§„ ë§í¬ ì°¾ê¸°"""
        broken_links = []
        
        # ëª¨ë“  ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì—ì„œ ë§í¬ ì¶”ì¶œ ë° í™•ì¸
        for md_file in self.base_dir.rglob('*.md'):
            try:
                with open(md_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # ë§ˆí¬ë‹¤ìš´ ë§í¬ íŒ¨í„´ ì°¾ê¸°
                import re
                links = re.findall(r'\[([^\]]+)\]\(([^)]+)\)', content)
                
                for link_text, link_url in links:
                    if link_url.startswith('http'):
                        # ì™¸ë¶€ ë§í¬ëŠ” ê±´ë„ˆë›°ê¸° (ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼)
                        continue
                    elif link_url.startswith('/'):
                        # ì ˆëŒ€ ê²½ë¡œ ë§í¬ í™•ì¸
                        full_url = f"{self.site_url}{link_url}"
                        try:
                            response = requests.head(full_url, timeout=5)
                            if response.status_code >= 400:
                                broken_links.append(f"{md_file}: {link_url}")
                        except:
                            broken_links.append(f"{md_file}: {link_url}")
                    else:
                        # ìƒëŒ€ ê²½ë¡œ ë§í¬ í™•ì¸
                        link_path = md_file.parent / link_url
                        if not link_path.exists():
                            broken_links.append(f"{md_file}: {link_url}")
                            
            except Exception as e:
                logger.warning(f"âš ï¸ Error checking links in {md_file}: {e}")
        
        return broken_links

    def find_missing_assets(self) -> List[str]:
        """ëˆ„ë½ëœ ì—ì…‹ ì°¾ê¸°"""
        missing_assets = []
        
        # í•„ìˆ˜ íŒŒì¼ë“¤ í™•ì¸
        required_files = [
            'favicon.ico',
            'assets/images/og-default.png',
            'assets/images/logo.png',
            'assets/manifest.json'
        ]
        
        for required_file in required_files:
            file_path = self.base_dir / required_file
            if not file_path.exists():
                missing_assets.append(required_file)
        
        return missing_assets

    def analyze_content_quality(self) -> Dict[str, Any]:
        """ì½˜í…ì¸  í’ˆì§ˆ ë¶„ì„"""
        quality_metrics = {
            'avg_post_length': 0,
            'posts_without_tags': [],
            'posts_without_description': [],
            'readability_scores': []
        }
        
        if self.posts_dir.exists():
            total_words = 0
            post_count = 0
            
            for post_file in self.posts_dir.glob('*.md'):
                try:
                    with open(post_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    if content.startswith('---'):
                        parts = content.split('---', 2)
                        if len(parts) >= 3:
                            front_matter_data = yaml.safe_load(parts[1])
                            post_content = parts[2]
                            
                            # íƒœê·¸ í™•ì¸
                            if not front_matter_data.get('tags'):
                                quality_metrics['posts_without_tags'].append(post_file.name)
                            
                            # ì„¤ëª… í™•ì¸
                            if not front_matter_data.get('description'):
                                quality_metrics['posts_without_description'].append(post_file.name)
                            
                            # ë‹¨ì–´ ìˆ˜ ê³„ì‚°
                            word_count = len(post_content.split())
                            total_words += word_count
                            post_count += 1
                            
                except Exception as e:
                    logger.warning(f"âš ï¸ Error analyzing content quality for {post_file}: {e}")
            
            if post_count > 0:
                quality_metrics['avg_post_length'] = round(total_words / post_count)
        
        return quality_metrics

    def check_security(self) -> Dict[str, Any]:
        """ë³´ì•ˆ ì ê²€"""
        logger.info("ğŸ”’ Running security scan...")
        
        security_report = {
            'https_enabled': self.site_url.startswith('https://'),
            'security_headers': self.check_security_headers(),
            'dependency_vulnerabilities': self.check_dependencies(),
            'sensitive_files': self.check_sensitive_files()
        }
        
        return security_report

    def check_security_headers(self) -> Dict[str, Any]:
        """ë³´ì•ˆ í—¤ë” í™•ì¸"""
        try:
            response = requests.get(self.site_url)
            headers = response.headers
            
            security_headers = {
                'content_security_policy': 'Content-Security-Policy' in headers,
                'x_frame_options': 'X-Frame-Options' in headers,
                'x_content_type_options': 'X-Content-Type-Options' in headers,
                'strict_transport_security': 'Strict-Transport-Security' in headers,
                'referrer_policy': 'Referrer-Policy' in headers
            }
            
            return security_headers
        except Exception as e:
            return {'error': str(e)}

    def check_dependencies(self) -> Dict[str, Any]:
        """ì˜ì¡´ì„± ì·¨ì•½ì  í™•ì¸"""
        try:
            # bundle audit ì‹¤í–‰
            result = subprocess.run(['bundle', 'audit', 'check'], 
                                  capture_output=True, text=True, timeout=60)
            
            return {
                'vulnerabilities_found': result.returncode != 0,
                'output': result.stdout,
                'errors': result.stderr if result.returncode != 0 else None
            }
        except Exception as e:
            return {'error': str(e)}

    def check_sensitive_files(self) -> List[str]:
        """ë¯¼ê°í•œ íŒŒì¼ í™•ì¸"""
        sensitive_patterns = [
            '*.key',
            '*.pem',
            '*.p12',
            '.env',
            'config/database.yml',
            'config/secrets.yml'
        ]
        
        sensitive_files = []
        for pattern in sensitive_patterns:
            for file_path in self.base_dir.rglob(pattern):
                if file_path.is_file():
                    sensitive_files.append(str(file_path.relative_to(self.base_dir)))
        
        return sensitive_files

    def check_build_status(self) -> Dict[str, Any]:
        """ë¹Œë“œ ìƒíƒœ í™•ì¸"""
        logger.info("ğŸ”¨ Checking build status...")
        
        try:
            # Jekyll ë¹Œë“œ í…ŒìŠ¤íŠ¸
            result = subprocess.run(['bundle', 'exec', 'jekyll', 'build', '--dry-run'], 
                                  capture_output=True, text=True, timeout=60)
            
            return {
                'build_successful': result.returncode == 0,
                'output': result.stdout,
                'errors': result.stderr if result.returncode != 0 else None,
                'build_time': datetime.now().isoformat()
            }
        except Exception as e:
            return {
                'build_successful': False,
                'error': str(e),
                'build_time': datetime.now().isoformat()
            }

    def calculate_health_score(self, health_report: Dict[str, Any]) -> int:
        """ì „ì²´ ê±´ê°•ë„ ì ìˆ˜ ê³„ì‚°"""
        score = 0
        max_score = 100
        
        # ì‚¬ì´íŠ¸ ì ‘ê·¼ì„± (20ì )
        if health_report['site_availability']['status'] == 'online':
            score += 20
            if health_report['site_availability']['response_time_ms'] < 1000:
                score += 5  # ë³´ë„ˆìŠ¤
        
        # ì„±ëŠ¥ (20ì )
        lighthouse_score = health_report['performance'].get('lighthouse_score')
        if lighthouse_score:
            score += int(lighthouse_score * 0.2)
        
        # SEO (20ì )
        seo_checks = [
            health_report['seo_health']['sitemap_exists'].get('exists', False),
            health_report['seo_health']['robots_txt_exists'].get('exists', False),
            health_report['seo_health']['meta_tags_analysis'].get('has_title', False),
            health_report['seo_health']['meta_tags_analysis'].get('has_description', False)
        ]
        score += sum(seo_checks) * 5
        
        # ì½˜í…ì¸  ë¬´ê²°ì„± (20ì )
        content_score = 0
        if health_report['content_integrity']['posts_count']['total_posts'] > 0:
            content_score += 10
        if len(health_report['content_integrity']['broken_links']) == 0:
            content_score += 10
        score += content_score
        
        # ë³´ì•ˆ (10ì )
        if health_report['security_scan']['https_enabled']:
            score += 5
        if not health_report['security_scan']['dependency_vulnerabilities'].get('vulnerabilities_found', True):
            score += 5
        
        # ë¹Œë“œ ìƒíƒœ (10ì )
        if health_report['build_status']['build_successful']:
            score += 10
        
        return min(score, max_score)

    def save_health_report(self, health_report: Dict[str, Any]) -> None:
        """ê±´ê°• ë³´ê³ ì„œ ì €ì¥"""
        report_file = self.automation_dir / f"health_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(health_report, f, indent=2, ensure_ascii=False, default=str)
            
            logger.info(f"ğŸ“Š Health report saved: {report_file}")
            
            # ìµœì‹  ë³´ê³ ì„œ ë§í¬ ìƒì„±
            latest_report = self.automation_dir / 'latest_health_report.json'
            if latest_report.exists():
                latest_report.unlink()
            latest_report.symlink_to(report_file.name)
            
        except Exception as e:
            logger.error(f"âŒ Failed to save health report: {e}")

    def optimize_images(self, quality: int = 85) -> Dict[str, Any]:
        """ì´ë¯¸ì§€ ìµœì í™”"""
        logger.info("ğŸ–¼ï¸ Starting image optimization...")
        
        optimization_report = {
            'processed_images': 0,
            'total_size_before': 0,
            'total_size_after': 0,
            'optimized_files': [],
            'errors': []
        }
        
        if not self.images_dir.exists():
            logger.warning("âš ï¸ Images directory not found")
            return optimization_report
        
        for img_file in self.images_dir.rglob('*'):
            if img_file.is_file() and img_file.suffix.lower() in ['.jpg', '.jpeg', '.png']:
                try:
                    original_size = img_file.stat().st_size
                    optimization_report['total_size_before'] += original_size
                    
                    # ì´ë¯¸ì§€ ìµœì í™”
                    with Image.open(img_file) as img:
                        # EXIF ë°ì´í„° ì œê±° ë° ìµœì í™”
                        if img.mode in ('RGBA', 'LA'):
                            # PNGë¡œ ì €ì¥ (íˆ¬ëª…ë„ ìœ ì§€)
                            img.save(img_file, 'PNG', optimize=True)
                        else:
                            # JPEGë¡œ ì €ì¥
                            if img.mode != 'RGB':
                                img = img.convert('RGB')
                            img.save(img_file, 'JPEG', quality=quality, optimize=True)
                    
                    new_size = img_file.stat().st_size
                    optimization_report['total_size_after'] += new_size
                    optimization_report['processed_images'] += 1
                    
                    if new_size < original_size:
                        optimization_report['optimized_files'].append({
                            'file': str(img_file.relative_to(self.base_dir)),
                            'size_before': original_size,
                            'size_after': new_size,
                            'savings_percent': round((1 - new_size/original_size) * 100, 1)
                        })
                    
                except Exception as e:
                    optimization_report['errors'].append({
                        'file': str(img_file.relative_to(self.base_dir)),
                        'error': str(e)
                    })
                    logger.error(f"âŒ Error optimizing {img_file}: {e}")
        
        # í†µê³„ ê³„ì‚°
        if optimization_report['total_size_before'] > 0:
            savings_percent = (1 - optimization_report['total_size_after'] / optimization_report['total_size_before']) * 100
            optimization_report['total_savings_percent'] = round(savings_percent, 1)
            optimization_report['size_saved_mb'] = round((optimization_report['total_size_before'] - optimization_report['total_size_after']) / (1024 * 1024), 2)
        
        logger.info(f"âœ… Image optimization complete: {optimization_report['processed_images']} images processed")
        return optimization_report

    def backup_content(self) -> Dict[str, Any]:
        """ì½˜í…ì¸  ë°±ì—…"""
        logger.info("ğŸ’¾ Starting content backup...")
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_dir = self.automation_dir / 'backups' / timestamp
        backup_dir.mkdir(parents=True, exist_ok=True)
        
        backup_report = {
            'timestamp': timestamp,
            'backup_location': str(backup_dir),
            'backed_up_items': [],
            'total_size_mb': 0,
            'errors': []
        }
        
        # ë°±ì—…í•  ë””ë ‰í† ë¦¬/íŒŒì¼ ëª©ë¡
        backup_items = [
            ('_posts', 'Blog posts'),
            ('_layouts', 'Layout templates'),
            ('_includes', 'Include files'),
            ('_data', 'Data files'),
            ('assets', 'Assets'),
            ('_config.yml', 'Site configuration'),
            ('about.md', 'About page'),
            ('index.md', 'Home page')
        ]
        
        for item, description in backup_items:
            source_path = self.base_dir / item
            
            if source_path.exists():
                try:
                    if source_path.is_file():
                        # íŒŒì¼ ë°±ì—…
                        dest_path = backup_dir / item
                        dest_path.parent.mkdir(parents=True, exist_ok=True)
                        shutil.copy2(source_path, dest_path)
                        size = source_path.stat().st_size
                    else:
                        # ë””ë ‰í† ë¦¬ ë°±ì—…
                        dest_path = backup_dir / item
                        shutil.copytree(source_path, dest_path, dirs_exist_ok=True)
                        size = sum(f.stat().st_size for f in source_path.rglob('*') if f.is_file())
                    
                    backup_report['backed_up_items'].append({
                        'item': item,
                        'description': description,
                        'size_mb': round(size / (1024 * 1024), 2)
                    })
                    backup_report['total_size_mb'] += size / (1024 * 1024)
                    
                except Exception as e:
                    backup_report['errors'].append({
                        'item': item,
                        'error': str(e)
                    })
                    logger.error(f"âŒ Error backing up {item}: {e}")
        
        backup_report['total_size_mb'] = round(backup_report['total_size_mb'], 2)
        
        # ë°±ì—… ë³´ê³ ì„œ ì €ì¥
        report_file = backup_dir / 'backup_report.json'
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(backup_report, f, indent=2, ensure_ascii=False, default=str)
        except Exception as e:
            logger.error(f"âŒ Failed to save backup report: {e}")
        
        logger.info(f"âœ… Backup complete: {len(backup_report['backed_up_items'])} items backed up")
        return backup_report

    def generate_sitemap(self) -> Dict[str, Any]:
        """ì‚¬ì´íŠ¸ë§µ ìƒì„±"""
        logger.info("ğŸ—ºï¸ Generating sitemap...")
        
        try:
            # Jekyll ë¹Œë“œë¥¼ í†µí•´ ì‚¬ì´íŠ¸ë§µ ìƒì„±
            result = subprocess.run(['bundle', 'exec', 'jekyll', 'build'], 
                                  capture_output=True, text=True, timeout=120)
            
            if result.returncode == 0:
                sitemap_file = self.base_dir / '_site' / 'sitemap.xml'
                if sitemap_file.exists():
                    # ì‚¬ì´íŠ¸ë§µ í†µê³„
                    with open(sitemap_file, 'r', encoding='utf-8') as f:
                        sitemap_content = f.read()
                    
                    import xml.etree.ElementTree as ET
                    root = ET.fromstring(sitemap_content)
                    url_count = len(root.findall('.//{http://www.sitemaps.org/schemas/sitemap/0.9}url'))
                    
                    return {
                        'success': True,
                        'url_count': url_count,
                        'file_size': sitemap_file.stat().st_size,
                        'generated_at': datetime.now().isoformat()
                    }
                else:
                    return {
                        'success': False,
                        'error': 'Sitemap file not found after build'
                    }
            else:
                return {
                    'success': False,
                    'error': 'Jekyll build failed',
                    'output': result.stderr
                }
        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }

    def run_full_automation(self) -> Dict[str, Any]:
        """ì „ì²´ ìë™í™” ì‹¤í–‰"""
        logger.info("ğŸš€ Starting full blog automation...")
        
        automation_report = {
            'started_at': datetime.now().isoformat(),
            'health_check': None,
            'image_optimization': None,
            'backup': None,
            'sitemap_generation': None,
            'completed_at': None,
            'overall_success': False
        }
        
        try:
            # 1. ê±´ê°• ìƒíƒœ ì ê²€
            automation_report['health_check'] = self.check_site_health()
            
            # 2. ì´ë¯¸ì§€ ìµœì í™”
            automation_report['image_optimization'] = self.optimize_images()
            
            # 3. ì½˜í…ì¸  ë°±ì—…
            automation_report['backup'] = self.backup_content()
            
            # 4. ì‚¬ì´íŠ¸ë§µ ìƒì„±
            automation_report['sitemap_generation'] = self.generate_sitemap()
            
            automation_report['completed_at'] = datetime.now().isoformat()
            automation_report['overall_success'] = True
            
            logger.info("âœ… Full automation completed successfully")
            
        except Exception as e:
            automation_report['error'] = str(e)
            automation_report['completed_at'] = datetime.now().isoformat()
            logger.error(f"âŒ Automation failed: {e}")
        
        # ìë™í™” ë³´ê³ ì„œ ì €ì¥
        report_file = self.automation_dir / f"automation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(automation_report, f, indent=2, ensure_ascii=False, default=str)
        except Exception as e:
            logger.error(f"âŒ Failed to save automation report: {e}")
        
        return automation_report

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Blog Automation Tool')
    parser.add_argument('--action', choices=['health', 'optimize', 'backup', 'sitemap', 'full'], 
                       default='health', help='Action to perform')
    parser.add_argument('--config', default='_config.yml', help='Jekyll config file path')
    
    args = parser.parse_args()
    
    automation = BlogAutomation(args.config)
    
    if args.action == 'health':
        result = automation.check_site_health()
        print(f"Health Score: {result['overall_score']}/100")
    elif args.action == 'optimize':
        result = automation.optimize_images()
        print(f"Optimized {result['processed_images']} images")
    elif args.action == 'backup':
        result = automation.backup_content()
        print(f"Backed up {len(result['backed_up_items'])} items")
    elif args.action == 'sitemap':
        result = automation.generate_sitemap()
        print(f"Sitemap generation: {'Success' if result['success'] else 'Failed'}")
    elif args.action == 'full':
        result = automation.run_full_automation()
        print(f"Full automation: {'Success' if result['overall_success'] else 'Failed'}")

if __name__ == '__main__':
    main()